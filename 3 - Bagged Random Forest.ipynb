{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Ames Housing Dataset - Bagged Random Forest\n",
    "\n",
    "These notebooks represent the project submission for the course [Data and Web Mining](https://www.unive.it/data/course/337525) by Professor [Claudio Lucchese](https://www.unive.it/data/people/5590426) at [Ca' Foscari University of Venice](https://www.unive.it).\n",
    "\n",
    "---\n",
    "\n",
    "## Premises\n",
    "\n",
    "### Process\n",
    "\n",
    "Even though, ideally, the process of exploratory data analysis should leave the data untouched, I decided to perform changes whenever I had the chance to improve the quality of the data. This was crucial as the presence of outliers is very high and there is evidence of data incoherence. What I mean by this, is that even though it is believed that the best approach is to separate the *passive* analysis and the actual feature engineering phase, I personally believe it is possible to do both for each step. Namely, at any moment we can use the inference we make to modify the data in order to achieve the best version of the original dataset.\n",
    "\n",
    "### Requirements for the project\n",
    "\n",
    "The project needs some global variables to run, as this allows the automation of data saving and import.\n",
    "\n",
    "---\n",
    "\n",
    "## Structure of the notebooks\n",
    "\n",
    "The project has 5 notebooks:\n",
    "1. Introduction and Data Preparation\n",
    "    * Domain research, Context of Data\n",
    "    * Preliminary Dataset Overview\n",
    "    * Correction of possible errors and coherence check \n",
    "    * Some preliminary feature creation\n",
    "2. Exploratory Data Analysis \n",
    "    * Univariate, Bivariate and Multivariate analysis\n",
    "    * Outliers removal \n",
    "3. Data Encoding, Type Conversion and Feature Selection\n",
    "    * Encoding of Categorical Features\n",
    "    * Conversion of all types\n",
    "    * Feature Importance Assessment\n",
    "    * Feature Selection\n",
    "4. Bagged Random Forest Regression\n",
    "    * Data Rescaling\n",
    "    * Train, Validation, Test\n",
    "    * Hyperparameters Tuning\n",
    "    * Diagnostics and Evaluation\n",
    "5. Graph Neural Network Regression\n",
    "    * Data conversion to graph and edges creation\n",
    "    * Data Rescaling\n",
    "    * Train, Validation and Test loops\n",
    "    * Diagnostics and Evaluation\n",
    "\n",
    "**Gianmaria Pizzo - 872966@stud.unive.it**\n",
    "\n",
    "---\n",
    "\n",
    "## Structure of this notebook\n",
    "\n",
    "This notebook covers the following points\n",
    "* Data Rescaling\n",
    "* Train, Validation, Test\n",
    "* Hyperparameters Tuning\n",
    "* Diagnostics and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "As we know from https://towardsdatascience.com/one-hot-encoding-is-making-your-tree-based-ensembles-worse-heres-why-d64b282b5769\n",
    "\n",
    "Because this procedure generates several new variables, it is prone to causing a large problem (too many predictors) if the original column has a large number of unique values. Another disadvantage of one-hot encoding is that it produces multicollinearity among the various variables, lowering the model's accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Split 60/20/20\n",
    "X_train_80, X_test, y_train_80, y_test = train_test_split(df_train, df_target,\n",
    "                                                          test_size = 0.20, random_state = 33)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid  = train_test_split(X_train_80, y_train_80, \n",
    "                                                       test_size=0.25, random_state=42)\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for c in [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]:\n",
    "    # train and predict\n",
    "    model = SVC(C=c, kernel='poly')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # compute Accuracy\n",
    "    train_acc = accuracy_score(y_true = y_train, \n",
    "                               y_pred = model.predict(X_train))\n",
    "    valid_acc = accuracy_score(y_true = y_valid, \n",
    "                               y_pred = model.predict(X_valid))\n",
    "    print (\"C: {:8.3f} - Train Accuracy: {:.3f} - Validation Accuracy: {:.3f}\"\n",
    "           .format( c, train_acc, valid_acc) )\n",
    "    \n",
    "    accuracies += [ [valid_acc, c] ]\n",
    "\n",
    "best_accuracy, best_c = max(accuracies)\n",
    "print ( \"Best C:\", best_c )\n",
    "\n",
    "# here we are using both training and validation,\n",
    "# to exploit the most data\n",
    "model = SVC(C=best_c, kernel='poly')\n",
    "model.fit(X_train_80,y_train_80)\n",
    "\n",
    "test_acc = accuracy_score(y_true = y_test, \n",
    "                          y_pred = model.predict(X_test) )\n",
    "print (\"Test Accuracy: {:.3f}\".format(test_acc) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X_train_80, X_test, y_train_80, y_test = train_test_split(df_train, df_target,\n",
    "                                                          test_size = 0.20, random_state = 42)\n",
    "\n",
    "model = SVC()\n",
    "parameters = { 'C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0],\n",
    "                'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "        \n",
    "tuned_model = GridSearchCV(model, parameters, cv=5, verbose=0)\n",
    "tuned_model.fit(X_train_80, y_train_80)\n",
    "\n",
    "print (\"Best Score: {:.3f}\".format(tuned_model.best_score_) )\n",
    "print (\"Best Params: \", tuned_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame( tuned_model.cv_results_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = accuracy_score(y_true = y_test, \n",
    "                          y_pred = tuned_model.predict(X_test) )\n",
    "print (\"Test Accuracy: {:.3f}\".format(test_acc) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
